import tensorflow as tf
import tensorflow.keras.backend as K

def GradientPenalty(f, real, fake):
    def _interpolate(a, b):
        shape = [tf.shape(a)[0]] + [1] * (a.shape.ndims - 1)
        alpha = tf.random.uniform(shape=shape, minval=0., maxval=1.)
        inter = (alpha * a) + ((1 - alpha) * b) 
        inter.set_shape(a.shape)
        return inter

    x = _interpolate(real, fake)
    with tf.GradientTape() as tape:
        tape.watch(x)
        pred = tf.reduce_mean(f(x))
    grad = tape.gradient(pred, x)
    
    grad = tf.reshape(grad, [tf.shape(grad)[0], -1])
    norm = tf.norm(grad, axis=1)
    gp = tf.reduce_mean((norm - 1.0)**2)

    return gp

def FeatLoss(y_true, y_pred):
    # y_true: vgg features derived from a batch of images generated by the Generator (B, h, w, c)
    # y_pred: vgg features derived from a batch of HR images (B, h, w, c)
    # Feature Reconstruction Loss
    # See: Perceptual Losses for Real-Time Style Transfer and Super-Resolution
    shape = K.int_shape(y_pred)                         # (B, h, w, c)
    axis = [1, 2, 3]
    coef = shape[1] * shape[2] * shape[3]
    diff = K.square(y_true - y_pred)                    # (B, h, w, c)
    diff_norm = K.sqrt(K.sum(diff, axis=axis)) / coef   # (B, )
    return K.mean(diff_norm)

def MAE(y_true, y_pred):
    return K.mean(K.abs(y_true - y_pred))

def WLossPos(y_pred):
    return K.mean(y_pred)

def WLossNeg(y_pred):
    return K.mean(y_pred) * (-1.0)